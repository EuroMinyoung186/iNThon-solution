{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-upfNsOyC72_",
        "outputId": "a6f63b9c-5c2f-429c-fafa-8248b309834d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tbsuvY-R07FW"
      },
      "outputs": [],
      "source": [
        "# train data를 type 1과 type 2로 구분\n",
        "\n",
        "import jsonlines\n",
        "import json\n",
        "\n",
        "input_file_path = 'train.json'\n",
        "train_file_path = 'train_type1.jsonl'\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with jsonlines.open(train_file_path, mode='w') as writer:\n",
        "    for item in data['dataset']:\n",
        "      if(item['type']=='Type 1') :\n",
        "        writer.write(item)\n",
        "\n",
        "train_file_path = 'train_type2.jsonl'\n",
        "\n",
        "with jsonlines.open(train_file_path, mode='w') as writer:\n",
        "    for item in data['dataset']:\n",
        "      if(item['type']=='Type 2') :\n",
        "        writer.write(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W-_F_sg6DeH",
        "outputId": "1a24f5f3-964d-4622-800d-ac88be87e310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13448 16552\n"
          ]
        }
      ],
      "source": [
        "# train data에서 type 1, 2인 데이터가 각각 몇 개씩 있는지 구한다.\n",
        "\n",
        "with open('train.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "n_type1=0\n",
        "n_type2=0\n",
        "\n",
        "with jsonlines.open(eval_file_path, mode='w') as writer:\n",
        "    for item in data['dataset']:\n",
        "      if(item['type']=='Type 1') :\n",
        "        n_type1+=1\n",
        "      else : n_type2+=1\n",
        "\n",
        "print(n_type1, n_type2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjcT7Ij6SHnE"
      },
      "outputs": [],
      "source": [
        "# test.json에 test.jsonl을 생성\n",
        "\n",
        "import jsonlines\n",
        "import json\n",
        "\n",
        "input_file_path = 'test.json'\n",
        "test_file_path = 'test.jsonl'\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with jsonlines.open(test_file_path, mode='w') as writer:\n",
        "    for item in data['dataset']:\n",
        "        writer.write(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xXMbJLCQtrz",
        "outputId": "5fb2b21c-4d00-4d99-a714-67fb6a8c9bd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_dataset을 생성하고, size와 set의 단어 개수가 다른 데이터가 몇 개인지 구한다.\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "test_dataset = load_dataset('json', data_files='test.jsonl', split='train')\n",
        "\n",
        "total=1500 # total number of test data\n",
        "num=0\n",
        "for i in test_dataset :\n",
        "  num+=len(i['set'].split('#'))==i['size']\n",
        "\n",
        "print(num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xPstb8rY-xW"
      },
      "outputs": [],
      "source": [
        "# 3만 개의 train data에서 3천 개의 valid data를 추출\n",
        "# valid data에서 type 1, 2인 데이터의 비율은 3 : 1\n",
        "# 그러므로 type 1, 2인 valid data는 각각 2250, 750개\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "type1_dataset = load_dataset('json', data_files='train_type1.jsonl', split='train') # 2250 sampling\n",
        "type2_dataset = load_dataset('json', data_files='train_type2.jsonl', split='train') # 750 sampling\n",
        "\n",
        "type1_train, type1_valid = train_test_split(type1_dataset, test_size=2250, random_state=123)\n",
        "type2_train, type2_valid = train_test_split(type2_dataset, test_size=750, random_state=123)\n",
        "\n",
        "for key, value in type2_train.items():\n",
        "    if key in type1_train:\n",
        "        type1_train[key].extend(value)\n",
        "    else:\n",
        "        type1_train[key] = value\n",
        "\n",
        "for key, value in type2_valid.items():\n",
        "    if key in type1_valid:\n",
        "        type1_valid[key].extend(value)\n",
        "    else:\n",
        "        type1_valid[key] = value\n",
        "\n",
        "split_train_tmp=type1_train\n",
        "split_valid_tmp=type1_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mERyq4qee7dZ"
      },
      "outputs": [],
      "source": [
        "split_train=[]\n",
        "\n",
        "for i in range(27000) :\n",
        "   split_train.append({\"id\":split_train_tmp['id'][i], \"type\":split_train_tmp['type'][i], \"size\":split_train_tmp['size'][i], \"set\":split_train_tmp['set'][i], \"sentence\":split_train_tmp['sentence'][i]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRlEUWz2fn9c"
      },
      "outputs": [],
      "source": [
        "split_valid=[]\n",
        "\n",
        "for i in range(3000) :\n",
        "  split_valid.append({\"id\":split_valid_tmp['id'][i], \"type\":split_valid_tmp['type'][i], \"size\":split_valid_tmp['size'][i], \"set\":split_valid_tmp['set'][i], \"sentence\":split_valid_tmp['sentence'][i]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72XMpeplcM2r"
      },
      "outputs": [],
      "source": [
        "with open('split_train.json', 'w', encoding='UTF-8') as f :\n",
        "\tjson.dump(split_train, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8IXpFrac1Bg"
      },
      "outputs": [],
      "source": [
        "with open('split_valid.json', 'w', encoding='UTF-8') as f :\n",
        "\tjson.dump(split_valid, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1xAwKfkvcHs",
        "outputId": "ff49548c-0dd0-4ef7-e217-3e1f05bc391d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sampled_valid=random.sample(split_valid, 1000)\n",
        "\n",
        "num=0\n",
        "\n",
        "for i in split_valid :\n",
        "  if(i in sampled_valid) :\n",
        "    word_list=i['set'].split('#')\n",
        "    length=len(word_list)\n",
        "    if(length==1) :\n",
        "      continue\n",
        "    target_index=random.randint(0, length-1)\n",
        "    del word_list[target_index]\n",
        "    new_word_list=\"\"\n",
        "    for j in word_list :\n",
        "      new_word_list+=j+'#'\n",
        "    i['set']=new_word_list[:-1]\n",
        "    num+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyNGKhzEmfhA"
      },
      "outputs": [],
      "source": [
        "with open('complete_split_valid.json', 'w', encoding='UTF-8') as f :\n",
        "\tjson.dump(split_valid, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok8x78xNBs6s"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "word_infer_train=[]\n",
        "\n",
        "for i in split_train :\n",
        "  word_list=i['set'].split('#')\n",
        "  length=len(word_list)\n",
        "  if(length==1) :\n",
        "    continue\n",
        "  target_index=random.randint(0, length-1)\n",
        "\n",
        "  word=word_list[target_index]\n",
        "  del word_list[target_index]\n",
        "\n",
        "  word_infer_train.append({\"input\":word_list, \"output\":word})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCZsMjQCC9ar"
      },
      "outputs": [],
      "source": [
        "with open('word_infer_train.json', 'w', encoding='UTF-8') as f :\n",
        "\tjson.dump(word_infer_train, f, indent=2, ensure_ascii=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
